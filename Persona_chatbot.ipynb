{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persona chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "11luyZ0Mcc2p6l2nIFE31uE6nmm0yQXDd",
      "authorship_tag": "ABX9TyMzca405Tj7JyncyhhkmNdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f146cab391044f9bf1af947e2427c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f988027f78c4a648a9d5166529edebc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c03c118c86c47e9a99f9416cb2fef07",
              "IPY_MODEL_1b3471ee7c4c4e3f90d811051a03ac4e"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyaoranClone/Persona-Chatbot/blob/master/Persona_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cRfdffuypIT"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyZ_XbfTCgFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8855e60f-749e-4160-ca9d-43d4fc9608d3"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 16 07:15:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4kE9OVyDnyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f550917d-4fb2-4ea5-9737-5af172ede8f5"
      },
      "source": [
        "#install Apex\n",
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyLh1dN6CIoX"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRc_k_5w4wg"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "from itertools import chain\n",
        "from argparse import ArgumentParser\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from tqdm import tqdm, trange\n",
        "from tqdm import tnrange, notebook\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, DistributedSampler, SequentialSampler\n",
        "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
        "                                 GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLDrxn3u2SAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f24872-4fa6-4771-90e7-9af570dfff14"
      },
      "source": [
        "!pip install spacy ftfy==4.4.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Collecting ftfy==4.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-4.4.3-cp36-none-any.whl size=41069 sha256=4fd874edf9ef42542fc6e354d3fb61ec22896f2c7dcff6784ccaeb3976c761c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-4.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQZZGVyS3N47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be346c30-9650-4a29-e9c0-a312ac76413f"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OIIpnG36uL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facf820d-35ad-4dd7-f53c-b4de5a91b118"
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSBJz8H2IA_"
      },
      "source": [
        "#According to Huggingface Convai tutorial\n",
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
        "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La6zq6rK48c3"
      },
      "source": [
        "#define hyperparameters\n",
        "class args:\n",
        "  model_checkpoint = 'openai-gpt'\n",
        "  device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  lr  = 6.25e-5\n",
        "  num_candidates = 4\n",
        "  personality_permutations = 2\n",
        "  num_history  = 2 #Number of previous exchanges to keep in history\n",
        "  fp16_training = \"O1\" #Set to O0, O1, O2 or O3 for fp16 training\n",
        "  train_batch_size = 2\n",
        "  valid_batch_size = 2\n",
        "  num_epochs= 2\n",
        "  no_sample=False\n",
        "  lm_coef = 2.0\n",
        "  mc_coef = 1.0\n",
        "  max_norm= 1.0\n",
        "  top_p = 0.9 # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\n",
        "  top_k = 0 # Filter top-k tokens before sampling (<=0: no filtering)\n",
        "  temperature = 0.7 # Sampling softmax temperature\n",
        "  max_len = 20 #Maximum length of the output utterances\n",
        "  min_len = 1\n",
        "  num_gpu = torch.cuda.device_count() #1\n",
        "  gradient_accumulation_steps= 4\n",
        "  local_rank= -1 # for distributed training\n",
        "  url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "  local_path= \"/content/drive/My Drive/Colab Notebooks/Dataset/personachat_self_original.json\"\n",
        "  dataset_cache_path = \"/content/drive/My Drive/Colab Notebooks/Dataset/dataset_cache/persona_cache.bin_GPTTokenizer\"\n",
        "  saved_dir = \"/content/drive/My Drive/Colab Notebooks/Trained Models/persona_chatbot/\"\n",
        "  longest_common = 5\n",
        "args = args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpGazIhUBqU"
      },
      "source": [
        "class FacebookPersonaDataset():\n",
        "  \"\"\"\n",
        "    Concatenate context segments in a single sequence [[bos+persona], [history], [reply+eos]]\n",
        "    Tokenize and convert them to tensor\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self,url,cache_path,tokenizer = ''):\n",
        "    self.file_path = url\n",
        "    self.tokenizer = tokenizer\n",
        "    self.cache_path = cache_path\n",
        "\n",
        "  def load_dataset(self):\n",
        "    #self.cache_path = self.cache_path + '_' + type(self.tokenizer).__name__  # To avoid using GPT cache for GPT-2 and vice-versa\n",
        "    if self.cache_path and os.path.isfile(self.cache_path):\n",
        "      #load tokenized dataset\n",
        "      dataset = torch.load(self.cache_path)\n",
        "      print(\"dataset loaded\")\n",
        "    else:\n",
        "      with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.loads(f.read())\n",
        "      \n",
        "      def tokenize(obj):\n",
        "        if isinstance(obj,str):\n",
        "          return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(obj))\n",
        "        if isinstance(obj,dict):\n",
        "          return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "        return list(tokenize(o) for o in obj)\n",
        "      dataset = tokenize(dataset)\n",
        "      torch.save(dataset,self.cache_path)\n",
        "    return dataset\n",
        "\n",
        "  def _pad_dataset(self,dataset, padding=0):\n",
        "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
        "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "    return dataset\n",
        "\n",
        "  def build_input(self,persona,history,reply,lm_labels = False,with_eos = True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "    #bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
        "    #sequence = [[bos] + list(persona) + [\"his\"] + history + [\"rep\"] + [reply]]\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])] #add bos, eos\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])] #add speaker1, speaker2\n",
        "    #after concat: [[bos+persona], [history], [reply+eos]]\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    #segment\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    #position\n",
        "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "    #language model labels is used to calculate lm_loss\n",
        "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"]) #labels set to -100 are ignored (masked)\n",
        "    if lm_labels:\n",
        "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "    return instance\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "    personachat_dataset = self.load_dataset()\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for name,dataset in personachat_dataset.items():\n",
        "      num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"]) #num_candidates are same for all dialoges\n",
        "      if args.num_candidates > 0 and name == 'train':\n",
        "        num_candidates =  min(num_candidates ,args.num_candidates)\n",
        "      for dialoge in dataset:\n",
        "        persona = dialoge[\"personality\"].copy()\n",
        "        for _ in range(args.personality_permutations):\n",
        "          for utterance in dialoge[\"utterances\"]:\n",
        "            history = utterance[\"history\"][-(2*args.num_history+1):]\n",
        "            for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "              #The last candidate is a ground truth reponse.\n",
        "              lm_labels = bool(j==num_candidates-1)\n",
        "              instance = self.build_input(persona,history,candidate,lm_labels)\n",
        "              for input_name,data in instance.items():\n",
        "                datasets[name][input_name].append(data) #datasets['train']['input_ids'] of [[c1 in u1],[c2 in u1],..,[c1 in u 7][c2 in u7]]\n",
        "            datasets[name][\"mc_labels\"].append(num_candidates - 1) #7\n",
        "            datasets[name][\"n_candidates\"] = num_candidates\n",
        "          persona = [persona[-1]] + persona[:-1]  # permuted personalitie\n",
        "          \n",
        "    #pad input and convert to tensor\n",
        "    print(\"pad input and convert to tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "      dataset =  self._pad_dataset(dataset, padding=self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "      for input_name in MODEL_INPUTS:\n",
        "        tensor =  torch.tensor(dataset[input_name])\n",
        "        if input_name != \"mc_labels\":\n",
        "          tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "        tensor_datasets[dataset_name].append(tensor)\n",
        "\n",
        "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    return train_dataset, valid_dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WetWt820zx"
      },
      "source": [
        "def add_special_tokens(model,tokenizer):\n",
        "  origin_num_tokens = len(tokenizer.encoder)\n",
        "  num_special_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "  if num_special_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=origin_num_tokens + num_special_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzKolTBxBuE"
      },
      "source": [
        "model_class_name = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
        "tokenizer_class_name = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
        "model = model_class_name.from_pretrained(args.model_checkpoint)\n",
        "tokenizer = tokenizer_class_name.from_pretrained(args.model_checkpoint)\n",
        "model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcS_NATZ11B-"
      },
      "source": [
        "add_special_tokens(model,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u08d7LQuJ2r2"
      },
      "source": [
        "def train(train_dataset,valid_dataset,model,tokenizer):\n",
        "  train_sampler = RandomSampler(train_dataset) if args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  train_loader = DataLoader(train_dataset,batch_size=args.train_batch_size,sampler=train_sampler)\n",
        "  optimizer = AdamW(model.parameters(),lr = args.lr,correct_bias=True)\n",
        "\n",
        "  if args.fp16_training:\n",
        "    from apex import amp\n",
        "    # Allow Amp to perform casts as required by the opt_level \n",
        "    model,optimizer = amp.initialize(model,optimizer,opt_level=args.fp16_training)\n",
        "  #Linearly decrease the learning rate from lr to zero\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = args.num_epochs* len(train_loader))\n",
        "  tr_loss, loss, last_saved_loss = 0.0, 0.0, 2.0\n",
        "  global_step = 0\n",
        "  metrics = {\n",
        "      \"nll\" : 1000.0,\n",
        "      \"accuracy\": 0.0\n",
        "  }\n",
        "  model.zero_grad() # Reset gradient tensor\n",
        "  if args.num_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "  if args.local_rank != -1:\n",
        "    model = DistributedDataParallel(model,device_ids = [args.local_rank],output_device=args.local_rank,find_unused_parameters=True)\n",
        "  \n",
        "  #eval model when we start the training and at the end of each epoch\n",
        "  if args.local_rank in [-1,0]:\n",
        "    metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "  for _ in range(args.num_epochs):\n",
        "    for step,batch in enumerate(notebook.tqdm(train_loader,disable= args.local_rank not in [-1,0])):\n",
        "      model.train()\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch )\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "\n",
        "      (lm_loss),(mc_loss), *_ = model(input_ids,token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,mc_labels=mc_labels,lm_labels=lm_labels)\n",
        "\n",
        "      loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef)/args.gradient_accumulation_steps # Normalize our loss (if averaged)\n",
        "\n",
        "      if args.num_gpu > 1:\n",
        "        # mean() to average on multi-gpu parallel training\n",
        "        loss = loss.mean()\n",
        "\n",
        "      if step % 1000 == 0:\n",
        "        print(\"Loss for step {} is {}\".format(step, loss))\n",
        "        #Save model\n",
        "        if last_saved_loss > loss:\n",
        "          model_to_save = model.module if hasattr(model, \"module\") else model #Take care of distributed/parallel training\n",
        "          model_to_save.save_pretrained(args.saved_dir)\n",
        "          tokenizer.save_pretrained(args.saved_dir)\n",
        "          last_saved_loss = loss\n",
        "\n",
        "\n",
        "      if args.fp16_training:\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "          scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
        "      else:\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),args.max_norm)\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      global_step += 1\n",
        "      if (step+1) % args.gradient_accumulation_steps == 0: # Wait a several backward step\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "        # if args.local_rank in [-1,0]:\n",
        "        #   metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "      # Update the learning rate.\n",
        "      scheduler.step()\n",
        "    #eval model \n",
        "    if args.local_rank in [-1,0]:\n",
        "      metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "        \n",
        "  return tr_loss/global_step,metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOZjHfN9K07L"
      },
      "source": [
        "def evaluate(model,valid_dataset,metrics,tokenizer):\n",
        "  valid_sample = SequentialSampler(valid_dataset) if  args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  eval_dataloader = DataLoader(valid_dataset,sampler=valid_sample,batch_size=args.valid_batch_size)\n",
        "\n",
        "  print(' * Running Evaluation')\n",
        "  print(' * Num of examples: %d\" ',len(valid_dataset))\n",
        "  print(' * Batch size: %d\" ',args.valid_batch_size)\n",
        "  nlls = None\n",
        "  accs = None\n",
        "  for step,batch in enumerate(notebook.tqdm(eval_dataloader,desc=\"Evaluating\")):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "      lm_logits, mc_logits, *_ = model(\n",
        "          input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids)\n",
        "      \n",
        "      lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
        "      lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
        "            \n",
        "      x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
        "      #convert pytorch tensor to numpy array to calculate\n",
        "      nll = torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]).detach().cpu().numpy()\n",
        "      acc = torch.sum((torch.max(x[0][1], 1)[1] == x[1][1]).int()).detach().cpu().numpy().mean()\n",
        "\n",
        "    if nlls is None:\n",
        "      nlls = nll\n",
        "      accs = acc\n",
        "    else:\n",
        "      nlls = np.append(nlls, nll)\n",
        "      accs = np.append(accs, acc)\n",
        "  \n",
        "  nlls_mean = np.mean(nlls)\n",
        "  accs_mean = np.mean(accs)\n",
        "    \n",
        "  if accs_mean>metrics['accuracy'] and nlls_mean<metrics['nll']:\n",
        "    print(\" * New high accuracy and nll! {} {} \".format(accs_mean, nlls_mean))\n",
        "    metrics.update({'nll': nlls_mean, 'accuracy': accs_mean})\n",
        "    #Save model if getting high accuracy\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model #Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.saved_dir)\n",
        "    tokenizer.save_pretrained(args.saved_dir)\n",
        "    \n",
        "  return metrics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhPSl9HHCzB"
      },
      "source": [
        "persona_dataset = FacebookPersonaDataset(args.local_path,args.dataset_cache_path,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tekpUzGE-lEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c53390d-77b1-47f7-f220-0b633dd89564"
      },
      "source": [
        "train_dataset, valid_dataset = persona_dataset.get_data_loaders() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded\n",
            "pad input and convert to tensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY8Pw_6oSWsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d591c918-ebfc-4f33-b710-d56f25c87531"
      },
      "source": [
        "train_dataset.tensors[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([262876, 4, 282])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHpSqEnMV2mU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "0f146cab391044f9bf1af947e2427c36",
            "0f988027f78c4a648a9d5166529edebc",
            "6c03c118c86c47e9a99f9416cb2fef07",
            "1b3471ee7c4c4e3f90d811051a03ac4e",
            "6834bc13cbfa4c9d84145140be13ba9e"
          ]
        },
        "outputId": "5397bb5d-676f-45da-e937-0518cf5c7858"
      },
      "source": [
        "total_loss, metric = train(train_dataset,valid_dataset,model,tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
            " * Running Evaluation\n",
            " * Num of examples: %d\"  15602\n",
            " * Batch size: %d\"  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f146cab391044f9bf1af947e2427c36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=7801.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " * New high accuracy and nll! 0.0917831047301628 5.24331521987915 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6834bc13cbfa4c9d84145140be13ba9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=131438.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_openai.py:689: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss for step 0 is 2.730668306350708\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 1000 is 2.8442227840423584\n",
            "Loss for step 2000 is 1.5790939331054688\n",
            "Loss for step 3000 is 2.2540135383605957\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 4000 is 1.1778292655944824\n",
            "Loss for step 5000 is 1.6517231464385986\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 6000 is 1.6394068002700806\n",
            "Loss for step 7000 is 1.6226752996444702\n",
            "Loss for step 8000 is 1.0814038515090942\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 9000 is 1.7652204036712646\n",
            "Loss for step 10000 is 1.3035157918930054\n",
            "Loss for step 11000 is 1.8181471824645996\n",
            "Loss for step 12000 is 1.609025239944458\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 13000 is 1.1896573305130005\n",
            "Loss for step 14000 is 2.5982749462127686\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 15000 is 1.1315021514892578\n",
            "Loss for step 16000 is 1.6054801940917969\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "Loss for step 17000 is 1.0692919492721558\n",
            "Loss for step 18000 is 1.2963300943374634\n",
            "Loss for step 19000 is 1.5230019092559814\n",
            "Loss for step 20000 is 1.5406818389892578\n",
            "Loss for step 21000 is 1.5366566181182861\n",
            "Loss for step 22000 is 1.1144263744354248\n",
            "Loss for step 23000 is 1.596925973892212\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 24000 is 1.5749019384384155\n",
            "Loss for step 25000 is 1.1158547401428223\n",
            "Loss for step 26000 is 1.8606213331222534\n",
            "Loss for step 27000 is 2.321924924850464\n",
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55VYUukYLQ99"
      },
      "source": [
        "#Interact\n",
        "def top_filtering(logits, top_k = 0, top_p = 0.9,threshold = -float('Inf'),filter_value=-float('Inf')):\n",
        "  \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "            threshold: a minimal threshold to keep logits\n",
        "  \"\"\"\n",
        "  assert logits.dim() == 1 #batch_size = 1\n",
        "  top_k = min(top_k,logits.size(-1))\n",
        "  if top_k > 0:\n",
        "    # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "    indices_to_remove = logits < torch.top_k(logits,top_k)[0][...,-1,None]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  if top_p > 0.0:\n",
        "    # Compute cumulative probabilities of sorted tokens\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    # Remove tokens with cumulative probability above the threshold\n",
        "    sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "    # Shift the indices to the right to keep also the first token above the threshold\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    # Back to unsorted indices and set them to -infinity\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  indices_to_remove = logits < threshold\n",
        "  logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oA7fgrJ4BJV"
      },
      "source": [
        "def sample_sequence(personality,history,tokenizer,model,persona_dataset,current_output = None):\n",
        "  \"\"\"\n",
        "    Generate reponse from previous reponses\n",
        "  \"\"\"\n",
        "  special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "  if current_output is None:\n",
        "    current_output = []\n",
        "  \n",
        "  for i in range (args.max_len):\n",
        "    instance = persona_dataset.build_input(personality,history,current_output,with_eos=False)\n",
        "    input_ids = torch.tensor(instance[\"input_ids\"],device= args.device).unsqueeze(0)\n",
        "    token_type_ids =  torch.tensor(instance[\"token_type_ids\"], device= args.device).unsqueeze(0)\n",
        "    \n",
        "    logits = model(input_ids,token_type_ids = token_type_ids)\n",
        "    if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "      logits = logits[0]\n",
        "    logits = logits[0] \n",
        "    # logits shape (batch_size, num_choices, sequence_length, vocab_size)\n",
        "    logits = logits[0,-1,:]/args.temperature \n",
        "    logits = top_filtering(logits,top_k = args.top_k,top_p = args.top_p)\n",
        "    probs = F.softmax(logits, -1)\n",
        "\n",
        "    prev = torch.topk(probs,1)[1] if args.no_sample else torch.multinomial(probs,1)\n",
        "    \n",
        "    if i < args.min_len and prev.item() in special_tokens_ids:\n",
        "      while prev.item() in special_tokens_ids:\n",
        "        if probs.max().item() == 1:\n",
        "          warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "          break  # avoid infinitely looping over special token\n",
        "        prev = torch.multinomial(probs, num_samples=1)\n",
        "    \n",
        "    if prev.item() in special_tokens_ids:\n",
        "      break\n",
        "    current_output.append(prev.item())\n",
        "      \n",
        "  return current_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgpm4NBkD1Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbf48ac-fd74-4642-836c-e3189e9bf4aa"
      },
      "source": [
        "dataset = persona_dataset.load_dataset()\n",
        "personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "personality = random.choice(personalities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W41VRCESB5BX"
      },
      "source": [
        "def predict(model,tokenizer,personality,persona_dataset):\n",
        "  print(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\n",
        "  history = []\n",
        "  out_ids = []\n",
        "  while True:\n",
        "    raw_text = input(\">>> \")\n",
        "    while not raw_text:\n",
        "      print('Prompt should not be empty!')\n",
        "      raw_text = input(\">>> \")\n",
        "    history.append(tokenizer.encode(raw_text))\n",
        "    with torch.no_grad():\n",
        "      #out_ids = sample_sequence(personality, history, tokenizer, model, persona_dataset) \n",
        "      while True:\n",
        "        out_ids = sample_sequence(personality, history, tokenizer, model, persona_dataset) \n",
        "        print(\"out_ids: \", out_ids)\n",
        "        if not check_repetition_cross_turn(history,out_ids):\n",
        "          break\n",
        "        else:\n",
        "          print(\"long common: \", tokenizer.decode(out_ids, skip_special_tokens=True))\n",
        "          if not out_ids: # avoid infinitely looping over special token\n",
        "            break\n",
        "    history.append(out_ids)\n",
        "    history = history[-(2*args.num_history+1):]\n",
        "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    print(out_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXPdvLH2B5PF"
      },
      "source": [
        "def check_repetition_cross_turn(history,output):\r\n",
        "  if not output:\r\n",
        "    return True\r\n",
        "  for seq in history:\r\n",
        "    if lcs(seq,output) >= args.longest_common:\r\n",
        "      return True\r\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q72D6vrvbPn"
      },
      "source": [
        "def lcs(X, Y): \n",
        "    # find the length of the strings \n",
        "    m = len(X) \n",
        "    n = len(Y) \n",
        "  \n",
        "    # declaring the array for storing the dp values \n",
        "    L = [[None]*(n + 1) for i in range(m + 1)] \n",
        "    # print(L)\n",
        "  \n",
        "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion \n",
        "    Note: L[i][j] contains length of LCS of X[0..i-1] \n",
        "    and Y[0..j-1]\"\"\"\n",
        "    for i in range(m + 1): \n",
        "        for j in range(n + 1): \n",
        "            if i == 0 or j == 0 : \n",
        "                L[i][j] = 0\n",
        "            elif X[i-1] == Y[j-1]: \n",
        "                L[i][j] = L[i-1][j-1]+1\n",
        "            else: \n",
        "                L[i][j] = max(L[i-1][j], L[i][j-1]) \n",
        "  \n",
        "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1] \n",
        "    return L[m][n] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX7TAP6FqomP"
      },
      "source": [
        "#reload checkpoints and evaluate on test dataset\n",
        "model_class_name = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
        "tokenizer_class_name = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
        "model = model_class_name.from_pretrained(args.saved_dir)\n",
        "tokenizer =  tokenizer_class_name.from_pretrained(args.saved_dir) #, do_lower_case=True)\n",
        "model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugt2phne1Ysq"
      },
      "source": [
        "#test\n",
        "lines = open(\"/content/drive/My Drive/Colab Notebooks/Dataset/personality.txt\").read().split('\\n')[:-1]\n",
        "lines = [line.lower() for line in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4N6SPgkBLdz"
      },
      "source": [
        "raw_personality = [\"my name is lisa .\",\"i like to remodel homes .\", \"i like to go hunting .\",\"i like to shoot a bow .\", \"my favorite holiday is halloween .\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIWqRrXICsv-"
      },
      "source": [
        "#convert to ids\n",
        "def tokenize(obj):\n",
        "  if isinstance(obj,str):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj,dict):\n",
        "    return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "  return list(tokenize(o) for o in obj)\n",
        "personality = tokenize(raw_personality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0Az_IQDMS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b5c6d8-38c3-4e28-dec9-778bc2145c82"
      },
      "source": [
        "personality"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[547, 1362, 544, 6544, 239],\n",
              " [249, 649, 485, 5371, 4621, 7654, 239],\n",
              " [249, 649, 485, 799, 5408, 239],\n",
              " [249, 649, 485, 4008, 246, 2877, 239],\n",
              " [547, 3898, 6974, 544, 13537, 239]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqu7XqQNBOiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "311c5df1-9b6a-4d8f-d7f0-c0fb46216225"
      },
      "source": [
        "tokenizer.decode(chain(*personality))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my name is lisa. i like to remodel homes. i like to go hunting. i like to shoot a bow. my favorite holiday is halloween.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9lMJYV8BK6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef993308-c7a9-4136-bf0d-e392e28c99b1"
      },
      "source": [
        "personality"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[72, 588, 284, 711, 9283, 764],\n",
              " [1820, 4004, 1295, 318, 262, 12269, 764],\n",
              " [72, 423, 362, 3988, 764],\n",
              " [72, 716, 257, 6253, 764]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaRlwCHtIlfe"
      },
      "source": [
        "personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "personality = random.choice(personalities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7drz1fXpjlk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6ea5430-2416-4c3b-c22e-ab39c9a117db"
      },
      "source": [
        "predict(model,tokenizer,personality,persona_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected personality: %s my name is lisa. i like to remodel homes. i like to go hunting. i like to shoot a bow. my favorite holiday is halloween.\n",
            ">>> hi\n",
            "out_ids:  [3569, 655, 240, 718, 640, 512, 257]\n",
            "hi there, how are you?\n",
            ">>> hi\n",
            "out_ids:  [3569, 267, 249, 1048, 862, 2190, 239, 718, 670, 512, 257]\n",
            "hi! i am well thanks. how about you?\n",
            ">>> hi\n",
            "out_ids:  [3569, 240, 249, 719, 3533, 240, 488, 249, 719, 246, 6070, 3300, 239]\n",
            "hi, i'm anne, and i'm a professional hunter.\n",
            ">>> hi\n",
            "out_ids:  [249, 719, 6544, 488, 249, 1119, 485, 4008, 8493]\n",
            "i'm lisa and i love to shoot deer\n",
            ">>> hi\n",
            "out_ids:  [249, 719, 6544, 240, 488, 249, 1119, 485, 5371, 4621, 7654, 239]\n",
            "long common:  i'm lisa, and i love to remodel homes.\n",
            "out_ids:  [249, 719, 4971]\n",
            "i'm bob\n",
            ">>> hi\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "what do you do for a living?\n",
            ">>> hi\n",
            "out_ids:  [249, 1048, 246, 3337, 4721, 2351]\n",
            "i am a computer programmer\n",
            ">>> hi\n",
            "out_ids:  [249, 719, 6544]\n",
            "i'm lisa\n",
            ">>> i'm lisa\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885]\n",
            "long common:  what do you do for a living\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "long common:  what do you do for a living?\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "long common:  what do you do for a living?\n",
            "out_ids:  [718, 640, 512]\n",
            "how are you\n",
            ">>> how are you\n",
            "out_ids:  [249, 719, 668, 2232, 30678, 484, 246, 1114]\n",
            "i'm just finished remodeling a house\n",
            ">>> i'm just finished remodeling a house\n",
            "out_ids:  [599, 544, 704, 3898, 7033, 257]\n",
            "what is your favorite season?\n",
            ">>> what is your favorite season?\n",
            "out_ids:  [1519, 544, 4478, 239]\n",
            "mine is spring.\n",
            ">>> mine is spring.\n",
            "out_ids:  [249, 1048, 246, 4572, 498, 4423, 620, 249, 719, 595, 834, 889, 498, 246, 4423, 1800, 239]\n",
            "i am a fan of winter so i'm not too much of a winter person.\n",
            ">>> i am a fan of winter so i'm not too much of a winter person.\n",
            "out_ids:  [249, 1048, 246, 1393, 4572, 498, 4075, 239]\n",
            "long common:  i am a big fan of christmas.\n",
            "out_ids:  [249, 1359, 649, 485, 5304, 488, 5532, 239]\n",
            "i also like to hunt and paint.\n",
            ">>> i also like to hunt and paint.\n",
            "out_ids:  [249, 587, 525, 504, 13686, 239, 249, 1359, 5371, 4621, 7654, 239]\n",
            "i do that on holidays. i also remodel homes.\n",
            ">>> i do that on holidays. i also remodel homes.\n",
            "out_ids:  [507, 544, 2664, 239]\n",
            "it is fun.\n",
            ">>> it is fun.\n",
            "out_ids:  [599, 252, 704, 3898, 7033, 257]\n",
            "what s your favorite season?\n",
            ">>> what s your favorite season?\n",
            "out_ids:  [1519, 544, 4478, 239, 249, 1119, 4075, 239]\n",
            "mine is spring. i love christmas.\n",
            ">>> mine is spring. i love christmas.\n",
            "out_ids:  [249, 1119, 4423, 240, 547, 3898, 6974, 239]\n",
            "i love winter, my favorite holiday.\n",
            ">>> i love winter, my favorite holiday.\n",
            "out_ids:  [599, 544, 704, 3898, 7033, 257]\n",
            "long common:  what is your favorite season?\n",
            "out_ids:  [599, 544, 704, 3898, 7033, 257]\n",
            "long common:  what is your favorite season?\n",
            "out_ids:  [718, 670, 512, 257]\n",
            "how about you?\n",
            ">>> how about you?\n",
            "out_ids:  [249, 1048, 6544, 239]\n",
            "i am lisa.\n",
            ">>> i am lisa.\n",
            "out_ids:  [599, 640, 704, 31054, 257]\n",
            "what are your hobbies?\n",
            ">>> what are your hobbies?\n",
            "out_ids:  [5408, 240, 5408, 240, 488, 5408, 239]\n",
            "hunting, hunting, and hunting.\n",
            ">>> hunting, hunting, and hunting\n",
            "out_ids:  [599, 640, 704, 31054, 257]\n",
            "long common:  what are your hobbies?\n",
            "out_ids:  [547, 3898]\n",
            "my favorite\n",
            ">>> my favorite\n",
            "out_ids:  [249, 719, 246, 5353, 3300, 240, 599, 544, 704, 3898, 615, 485, 5304, 257]\n",
            "i'm a former hunter, what is your favorite thing to hunt?\n",
            ">>> i'm a former hunter, what is your favorite thing to hunt?\n",
            "out_ids:  [249, 1119, 485, 799, 5408, 556, 547, 1463, 239]\n",
            "i love to go hunting with my family.\n",
            ">>> i love to go hunting with my family.\n",
            "out_ids:  [599, 252, 704, 3898, 615, 485, 587, 562, 2664, 257]\n",
            "long common:  what s your favorite thing to do for fun?\n",
            "out_ids:  [249, 5371, 4621, 7654, 239, 249, 1119, 525, 267]\n",
            "i remodel homes. i love that!\n",
            ">>> i remodel homes. i love that!\n",
            "out_ids:  [587, 512, 1894, 556, 704, 1463, 257]\n",
            "do you live with your family?\n",
            ">>> do you live with your family?\n",
            "out_ids:  [685, 240, 606, 640, 1629, 246, 1322, 498, 1129, 239]\n",
            "yes, we are actually a lot of work.\n",
            ">>> .\n",
            "out_ids:  [249, 5371, 4621, 7654, 246, 1322, 239]\n",
            "long common:  i remodel homes a lot.\n",
            "out_ids:  [249, 1048, 881, 512, 3545, 704, 1890, 239]\n",
            "i am sure you enjoy your job.\n",
            ">>> .\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "what do you do for a living?\n",
            ">>> .\n",
            "out_ids:  [249, 5371, 4621, 7654, 562, 246, 1885]\n",
            "i remodel homes for a living\n",
            ">>> .\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "long common:  what do you do for a living?\n",
            "out_ids:  [525, 544, 6702, 239, 249, 5371, 4621, 7654, 239]\n",
            "that is awesome. i remodel homes.\n",
            ">>> .\n",
            "out_ids:  [249, 719, 246, 3300, 239]\n",
            "i'm a hunter.\n",
            ">>> .\n",
            "out_ids:  [249, 1048, 246, 5648, 2467, 4297, 6487, 239]\n",
            "i am a video game developer.\n",
            ">>> .\n",
            "out_ids:  [249, 719, 246, 3337, 4297, 6487]\n",
            "i'm a computer developer\n",
            ">>> .\n",
            "out_ids:  [249, 5371, 4621, 7654, 488, 587, 507, 562, 246, 1885, 239]\n",
            "i remodel homes and do it for a living.\n",
            ">>> .\n",
            "out_ids:  [525, 535, 246, 2664, 18722]\n",
            "that's a fun hobby\n",
            ">>> \n",
            "Prompt should not be empty!\n",
            ">>> .\n",
            "out_ids:  [249, 719, 531, 26700, 3300, 488, 1119, 485, 5304]\n",
            "i'm an avid hunter and love to hunt\n",
            ">>> .\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "what do you do for a living?\n",
            ">>> ok\n",
            "out_ids:  [249, 5371, 4621, 7654]\n",
            "i remodel homes\n",
            ">>> ok\n",
            "out_ids:  [1439, 525, 544, 2548, 239]\n",
            "yeah that is cool.\n",
            ">>> ok\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "long common:  what do you do for a living?\n",
            "out_ids:  [249, 5371, 4621, 7654]\n",
            "i remodel homes\n",
            ">>> ok\n",
            "out_ids:  [249, 5371, 4621, 7654]\n",
            "i remodel homes\n",
            ">>> ok\n",
            "out_ids:  [249, 587, 5371, 4621, 7654]\n",
            "i do remodel homes\n",
            ">>> ok\n",
            "out_ids:  [249, 5371, 4621, 7654]\n",
            "i remodel homes\n",
            ">>> ok\n",
            "out_ids:  [249, 649, 485, 5304]\n",
            "i like to hunt\n",
            ">>> ok\n",
            "out_ids:  [249, 719, 6544]\n",
            "i'm lisa\n",
            ">>> ok\n",
            "out_ids:  [249, 649, 485, 799, 5408]\n",
            "i like to go hunting\n",
            ">>> \n",
            "Prompt should not be empty!\n",
            ">>> fuck\n",
            "out_ids:  [599, 544, 704, 1362]\n",
            "what is your name\n",
            ">>> khue\n",
            "out_ids:  [599, 252, 704, 1362]\n",
            "what s your name\n",
            ">>> khue\n",
            "out_ids:  [249, 1048, 6544]\n",
            "i am lisa\n",
            ">>> khue\n",
            "out_ids:  [587, 512, 649, 8493, 257]\n",
            "do you like deer?\n",
            ">>> khue\n",
            "out_ids:  [249, 649, 8493, 834]\n",
            "i like deer too\n",
            ">>> khue\n",
            "out_ids:  [1875, 239, 249, 719, 6544]\n",
            "nice. i'm lisa\n",
            ">>> khue\n",
            "out_ids:  [1875, 485, 1973, 512]\n",
            "nice to meet you\n",
            ">>> yes\n",
            "out_ids:  [1875, 485, 1973, 512, 834]\n",
            "nice to meet you too\n",
            ">>> too\n",
            "out_ids:  [599, 544, 704, 1362, 257]\n",
            "what is your name?\n",
            ">>> name\n",
            "out_ids:  [6955, 488, 249, 1119, 5408]\n",
            "todd and i love hunting\n",
            ">>> ting\n",
            "out_ids:  [1875, 485, 1973, 512, 834]\n",
            "long common:  nice to meet you too\n",
            "out_ids:  [1875, 485, 1973, 512, 834]\n",
            "long common:  nice to meet you too\n",
            "out_ids:  [1875, 485, 1973, 512, 834]\n",
            "long common:  nice to meet you too\n",
            "out_ids:  [1875, 485, 1973, 512, 834]\n",
            "long common:  nice to meet you too\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885, 257]\n",
            "what do you do for a living?\n",
            ">>> okay\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885]\n",
            "long common:  what do you do for a living\n",
            "out_ids:  [249, 5371, 4621, 7654]\n",
            "i remodel homes\n",
            ">>> okay\n",
            "out_ids:  [249, 649, 485, 5304, 488, 249, 649, 485, 5371, 4621, 7654]\n",
            "i like to hunt and i like to remodel homes\n",
            ">>> okay\n",
            "out_ids:  [599, 544, 704, 3898, 7033, 257]\n",
            "what is your favorite season?\n",
            ">>> yup\n",
            "out_ids:  [547, 3898, 7033, 544, 3111, 912, 249, 649, 485, 2425, 501, 1479, 498, 5089]\n",
            "my favorite season is summer because i like to eat alot of meat\n",
            ">>> yup\n",
            "out_ids:  [249, 1119, 5408]\n",
            "i love hunting\n",
            ">>> no\n",
            "out_ids:  [249, 1119, 485, 4008]\n",
            "i love to shoot\n",
            ">>> dunno\n",
            "out_ids:  [249, 1048, 834, 3004]\n",
            "i am too busy\n",
            ">>> okay\n",
            "out_ids:  [587, 512, 604, 775, 11370]\n",
            "do you have any pets\n",
            ">>> yes\n",
            "out_ids:  [249, 587]\n",
            "i do\n",
            ">>> what is your pets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Warning: model generating special token with probability 1.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "out_ids:  []\n",
            "long common:  \n",
            "\n",
            ">>> okay\n",
            "out_ids:  [599, 587, 512, 587, 562, 246, 1885]\n",
            "what do you do for a living\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-75c43f682c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersonality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersona_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-83f6fd927809>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, tokenizer, personality, persona_dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mout_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prompt should not be empty!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykv4heIDMeuP"
      },
      "source": [
        "\r\n",
        "x = (([1,2,3],[[3,4,3],[5,8,5],[6,6,11]]),([3,4,3],[5,5,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBDJI7pjGKBz"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUMHXeNTMn_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcb8f9ab-880d-4586-a252-e80f9cf8d0cf"
      },
      "source": [
        "torch.sum((torch.max(torch.tensor(x[0][1]), 1)[1] == torch.tensor(x[1][1])).int()).detach().cpu().numpy().mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgvS84IAvbyJ",
        "outputId": "1bf5d251-8e1e-44be-bf2f-34e1c7e42c33"
      },
      "source": [
        "X = [1820, 1438, 318, 312, 313, 764]\n",
        "Y = [1820, 1438, 318, 300, 9160, 764]\n",
        "print(\"Length of LCS is \", lcs(X, Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[None, None, None, None, None, None, None], [None, None, None, None, None, None, None], [None, None, None, None, None, None, None], [None, None, None, None, None, None, None], [None, None, None, None, None, None, None], [None, None, None, None, None, None, None], [None, None, None, None, None, None, None]]\n",
            "Length of LCS is  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yAZmRbuvhiU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}