{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persona chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11luyZ0Mcc2p6l2nIFE31uE6nmm0yQXDd",
      "authorship_tag": "ABX9TyOYxSsAOFaSiwI9neW8mruH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyaoranClone/Persona-Chatbot/blob/master/Persona_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cRfdffuypIT"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4kE9OVyDnyD",
        "outputId": "1ad264e4-80fa-475a-df7f-d222aca819fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#install Apex\n",
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyLh1dN6CIoX"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRc_k_5w4wg"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from itertools import chain\n",
        "from argparse import ArgumentParser\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, DistributedSampler, SequentialSampler\n",
        "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
        "                                 GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OIIpnG36uL",
        "outputId": "cf5d17e7-4599-4723-e17f-0f0ad05ea8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSBJz8H2IA_"
      },
      "source": [
        "#According to Huggingface Convai tutorial\n",
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
        "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La6zq6rK48c3"
      },
      "source": [
        "#define hyperparameters\n",
        "class args:\n",
        "  model_checkpoint = 'gpt2'\n",
        "  device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  lr  = 6.25e-5\n",
        "  num_candidates = 2\n",
        "  num_history  = 2 #Number of previous exchanges to keep in history\n",
        "  fp16_training = \"O1\" #Set to O0, O1, O2 or O3 for fp16 training\n",
        "  train_batch_size = 2\n",
        "  valid_batch_size = 2\n",
        "  num_epochs= 3\n",
        "  lm_coef = 2.0\n",
        "  mc_coef = 1.0\n",
        "  max_norm= 1.0\n",
        "  num_gpu = torch.cuda.device_count() #1\n",
        "  gradient_accumulation_steps= 8\n",
        "  local_rank= -1 # for distributed training\n",
        "  url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "  local_path= \"/content/drive/My Drive/Colab Notebooks/Dataset/personachat_self_original.json\"\n",
        "  dataset_cache_path = \"/content/drive/My Drive/Colab Notebooks/Dataset/dataset_cache/persona_cache.bin_GPT2Tokenizer\"\n",
        "  saved_model_dir = \"/content/drive/My Drive/Colab Notebooks/Trained Models/persona_chatbot/\"\n",
        "\n",
        "args = args"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpGazIhUBqU"
      },
      "source": [
        "class FacebookPersonaDataset():\n",
        "  \"\"\"\n",
        "    Concatenate context segments in a single sequence [[bos+persona], [history], [reply+eos]]\n",
        "    Tokenize and convert them to tensor\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self,url,cache_path,tokenizer = ''):\n",
        "    self.file_path = url\n",
        "    self.tokenizer = tokenizer\n",
        "    self.cache_path = cache_path\n",
        "\n",
        "  def _load_dataset(self):\n",
        "    #self.cache_path = self.cache_path + '_' + type(self.tokenizer).__name__  # To avoid using GPT cache for GPT-2 and vice-versa\n",
        "    if self.cache_path and os.path.isfile(self.cache_path):\n",
        "      #load tokenized dataset\n",
        "      dataset = torch.load(self.cache_path)\n",
        "      print(\"dataset loaded\")\n",
        "    else:\n",
        "      with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.loads(f.read())\n",
        "      \n",
        "      def tokenize(obj):\n",
        "        if isinstance(obj,str):\n",
        "          return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(obj))\n",
        "        if isinstance(obj,dict):\n",
        "          return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "        return list(tokenize(o) for o in obj)\n",
        "      dataset = tokenize(dataset)\n",
        "      torch.save(dataset,self.cache_path)\n",
        "    return dataset\n",
        "\n",
        "  def _pad_dataset(self,dataset, padding=0):\n",
        "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
        "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "    return dataset\n",
        "\n",
        "  def _build_input(self,persona,history,reply,lm_labels = False,with_eos = True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "    #bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
        "    #sequence = [[bos] + list(persona) + [\"his\"] + history + [\"rep\"] + [reply]]\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])] #add bos, eos\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])] #add speaker1, speaker2\n",
        "    #after concat: [[bos+persona], [history], [reply+eos]]\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    #segment\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    #position\n",
        "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "    #language model labels is used to calculate lm_loss\n",
        "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"]) #labels set to -100 are ignored (masked)\n",
        "    if lm_labels:\n",
        "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "    return instance\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "    personachat_dataset = self._load_dataset()\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for name,dataset in personachat_dataset.items():\n",
        "      num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"]) #num_candidates are same for all dialoges\n",
        "      if args.num_candidates > 0 and name == 'train':\n",
        "        num_candidates =  min(num_candidates ,args.num_candidates)\n",
        "      for dialoge in dataset:\n",
        "        persona = dialoge[\"personality\"].copy()\n",
        "        for utterance in dialoge[\"utterances\"]:\n",
        "          history = utterance[\"history\"][-(2*args.num_history+1):]\n",
        "          for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "            #The last candidate is a ground truth reponse.\n",
        "            lm_labels = bool(j==num_candidates-1)\n",
        "            instance = self._build_input(persona,history,candidate,lm_labels)\n",
        "            for input_name,data in instance.items():\n",
        "              datasets[name][input_name].append(data) #datasets['train']['input_ids'] of [[c1 in u1],[c2 in u1],..,[c1 in u 7][c2 in u7]]\n",
        "          datasets[name][\"mc_labels\"].append(num_candidates - 1) #\n",
        "          datasets[name][\"n_candidates\"] = num_candidates\n",
        "\n",
        "    #pad input and convert to tensor\n",
        "    print(\"pad input and convert to tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "      dataset =  self._pad_dataset(dataset, padding=self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "      for input_name in MODEL_INPUTS:\n",
        "        tensor =  torch.tensor(dataset[input_name])\n",
        "        if input_name != \"mc_labels\":\n",
        "          tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "        tensor_datasets[dataset_name].append(tensor)\n",
        "\n",
        "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    return train_dataset, valid_dataset "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhPSl9HHCzB"
      },
      "source": [
        "persona_dataset = FacebookPersonaDataset(args.local_path,args.dataset_cache_path,tokenizer)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tekpUzGE-lEZ",
        "outputId": "5146579b-9fa3-458f-91f7-12984c41bfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_dataset, valid_dataset = persona_dataset.get_data_loaders() "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded\n",
            "pad input and convert to tensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY8Pw_6oSWsz",
        "outputId": "42136c3b-bbb6-4d83-e62a-7da55dafbe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset.tensors[0].shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([131438, 2, 280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WetWt820zx"
      },
      "source": [
        "def add_special_token(model,tokenizer):\n",
        "  origin_num_tokens = len(tokenizer.encoder)\n",
        "  num_special_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "  if num_special_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=origin_num_tokens + num_special_tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzKolTBxBuE"
      },
      "source": [
        "model = GPT2DoubleHeadsModel.from_pretrained(args.model_checkpoint)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(args.model_checkpoint)\n",
        "model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcS_NATZ11B-"
      },
      "source": [
        "add_special_token(model,tokenizer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u08d7LQuJ2r2"
      },
      "source": [
        "def train(train_dataset,valid_dataset,model,tokenizer):\n",
        "  train_sampler = RandomSampler(train_dataset) if args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  train_loader = DataLoader(train_dataset,batch_size=args.train_batch_size,sampler=train_sampler)\n",
        "  optimizer = AdamW(model.parameters(),lr = args.lr,correct_bias=True)\n",
        "\n",
        "  if args.fp16_training:\n",
        "    from apex import amp\n",
        "    # Allow Amp to perform casts as required by the opt_level \n",
        "    model,optimizer = amp.initialize(model,optimizer,opt_level=args.fp16_training)\n",
        "  # Linearly decrease the learning rate from lr to zero\n",
        "  #scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.num_epochs* len(train_loader), 0.0)])\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = args.num_epochs+1)\n",
        "  tr_loss, loss = 0.0, 0.0\n",
        "  global_step = 0\n",
        "  metrics = {\n",
        "      \"nll\" : 1000.0,\n",
        "      \"accuracy\": 0.0\n",
        "  }\n",
        "  if args.num_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "  if args.local_rank != -1:\n",
        "    model = DistributedDataParallel(model,device_ids = [args.local_rank],output_device=args.local_rank,find_unused_parameters=True)\n",
        "  for _ in range(args.num_epochs):\n",
        "    for step,batch in enumerate(tqdm(train_loader,disable= args.local_rank not in [-1,0])):\n",
        "      model.train()\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch )\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "\n",
        "      (lm_loss),(mc_loss), *_ = model(input_ids,token_type_ids=token_type_ids,mc_labels=mc_labels,lm_labels=lm_labels)\n",
        "\n",
        "      loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef)/args.gradient_accumulation_steps\n",
        "      if args.fp16_training:\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "          scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
        "      else:\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),args.max_norm)\n",
        "\n",
        "      if args.num_gpu > 1:\n",
        "        # mean() to average on multi-gpu parallel training\n",
        "        loss = loss.mean()\n",
        "      \n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss for step {} is {}\".format(step, loss))\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      global_step += 1\n",
        "      if (step+1) % args.gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        if args.local_rank in [-1,0]:\n",
        "          metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "        \n",
        "  return tr_loss/global_step,metrics\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOZjHfN9K07L"
      },
      "source": [
        "def evaluate(model,valid_dataset,metrics,tokenizer):\n",
        "  valid_sample = SequentialSampler(valid_dataset) if  args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  eval_dataloader = DataLoader(valid_dataset,sampler=valid_sample,batch_size=args.valid_batch_size)\n",
        "\n",
        "  print(' * Running Evaluation')\n",
        "  print(' * Num of examples: %d\" ',len(valid_dataset))\n",
        "  print(' * Batch size: %d\" ',args.valid_batch_size)\n",
        "  nlls = None\n",
        "  accs = None\n",
        "  for step,batch in enumerate(tqdm(eval_dataloader,desc=\"Evaluating\")):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "      lm_logits, mc_logits, *_ = model(\n",
        "          input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids)\n",
        "      \n",
        "      lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
        "      lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
        "            \n",
        "      x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
        "      #convert pytorch tensor to numpy array to calculate\n",
        "      nll = torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]).detach().cpu().numpy()\n",
        "      acc = torch.sum((torch.max(x[0][1], 1)[1] == x[1][1]).int()).detach().cpu().numpy().mean()\n",
        "\n",
        "    if nlls is None:\n",
        "      nlls = nll\n",
        "      accs = acc\n",
        "    else:\n",
        "      nlls = np.append(nlls, nll)\n",
        "      accs = np.append(accs, acc)\n",
        "  \n",
        "  nlls_mean = np.mean(nlls)\n",
        "  accs_mean = np.mean(accs)\n",
        "    \n",
        "  if accs_mean>metrics['accuracy'] and nlls_mean<metrics['nll']:\n",
        "    print(\" * New high accuracy and nll! {} {} \".format(accs_mean, nlls_mean))\n",
        "    metrics.update({'nll': nlls_mean, 'accuracy': accs_mean})\n",
        "    #Save model if getting high accuracy\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model #Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "    \n",
        "  return metrics "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SBlJBYkLSq9"
      },
      "source": [
        "train(train_dataset,valid_dataset,model,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55VYUukYLQ99",
        "outputId": "5d4d36da-76c4-4b07-dada-0b1dcfbbf992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#Interact\n",
        "def top_filtering(logits, top_k = 0, top_p = 0.9,threshold = -float('Inf'),filter_value=-float('Inf')):\n",
        "  \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "            threshold: a minimal threshold to keep logits\n",
        "  \"\"\"\n",
        "  assert logits.dim() == 1 #batch_size = 1\n",
        "  top_k = min(top_k,logits.size(-1))\n",
        "  if top_k > 0:\n",
        "    # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "    indices_to_remove = torch.top_k(logits,top_k)[0][...,-1,None]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  if top_p > 0.0:\n",
        "    # Compute cumulative probabilities of sorted tokens\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    # Remove tokens with cumulative probability above the threshold\n",
        "    sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "    # Shift the indices to the right to keep also the first token above the threshold\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    # Back to unsorted indices and set them to -infinity\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  indices_to_remove = logits < threshold\n",
        "  logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return logits"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-6518f2d6114a>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QYMiISTkc0j",
        "outputId": "975da4eb-a17c-4ea5-9533-b6897fafe72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.arange(1., 6.)\n",
        "torch.topk(x, 3)[0][...,-1,None]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykv4heIDMeuP"
      },
      "source": [
        "x = (([1,2,3],[[3,4,3],[5,8,5],[6,6,11]]),([3,4,3],[5,5,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBDJI7pjGKBz"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUMHXeNTMn_p",
        "outputId": "fcb8f9ab-880d-4586-a252-e80f9cf8d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.sum((torch.max(torch.tensor(x[0][1]), 1)[1] == torch.tensor(x[1][1])).int()).detach().cpu().numpy().mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}