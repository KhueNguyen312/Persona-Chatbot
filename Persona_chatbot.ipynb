{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persona chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11luyZ0Mcc2p6l2nIFE31uE6nmm0yQXDd",
      "authorship_tag": "ABX9TyOjkRDM6GPki96rP4rJfLqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyaoranClone/Persona-Chatbot/blob/master/Persona_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cRfdffuypIT"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4kE9OVyDnyD",
        "outputId": "bc4795ae-63f8-4c91-d9fb-9135d04aa6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#install Apex\n",
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./\n",
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRc_k_5w4wg"
      },
      "source": [
        "import json\n",
        "import time\n",
        "from itertools import chain\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
        "#                                  GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCqLpabZewNh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSBJz8H2IA_"
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWOi74ZzaPK"
      },
      "source": [
        "#define hyperparameters\n",
        "params = {}\n",
        "params[\"model_checkpoint\"]  = 'gpt2'\n",
        "params[\"device\"]  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "params[\"lr\"]  = 6.25e-5\n",
        "params[\"num_candidates\"]  = 2\n",
        "params[\"num_history\"]  = 2 #Number of previous exchanges to keep in history\n",
        "params[\"fp16_training\"] = \"O1\" #Set to O0, O1, O2 or O3 for fp16 training\n",
        "params[\"url\"]  = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "params[\"local_path\"] = \"/content/drive/My Drive/Colab Notebooks/Dataset/personachat_self_original.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpGazIhUBqU"
      },
      "source": [
        "class DataLoaders():\n",
        "  def __init__(self,url,tokenizer = ''):\n",
        "    self.file_path = url\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def _load_dataset(self):\n",
        "    with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      dataset = json.loads(f.read())\n",
        "    \n",
        "    # def tokenize(obj):\n",
        "    #   if isinstance(obj,str):\n",
        "    #     return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(obj))\n",
        "    #   if isinstance(obj,dict):\n",
        "    #     return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "    #   return list(tokenize(o) for o in obj)\n",
        "    # dataset = tokenize(dataset)\n",
        "    return dataset\n",
        "  \n",
        "  def _build_input(self,persona,history,reply,tokenizer,lm_labes = False,with_eos = True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])] #add bos, eos\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])] #add speaker1, speaker2\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    #segment\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    #position\n",
        "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "    #language model labels is used to calculate lm_loss\n",
        "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
        "    if lm_labels:\n",
        "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
        "    return instance\n",
        "\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "    personachat_dataset = self._load_dataset()\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for name,dataset in token_data.items():\n",
        "      for dialoge in dataset:\n",
        "        persona = dialoge[\"personality\"].copy()\n",
        "        for utterance in dialoge[\"utterances\"]:\n",
        "          history = utterance[\"history\"][-(2*params[\"num_history\"]+1):]\n",
        "          for j, candidate in enumerate(utterance[\"candidates\"][-params[\"num_candidates\"]:]):\n",
        "            #Return True if j is the last candidate\n",
        "            lm_labels = bool(j==params[\"num_candidates\"]-1)\n",
        "            instance = self._build_input(persona,history,candidate,lm_labels)\n",
        "            for input_name,data in instance.items():\n",
        "              datasets[name][input_name].append(data)\n",
        "          datasets[name][\"mc_labels\"].append(num_candidates - 1)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhPSl9HHCzB"
      },
      "source": [
        "dataloaders = DataLoaders(params[\"local_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvfMosliIwJA"
      },
      "source": [
        "token_data = dataloaders._load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEJtDwtKqH7"
      },
      "source": [
        "for name,dataset in token_data.items():\n",
        "  for dialoge in dataset:\n",
        "    persona = dialoge[\"personality\"].copy()\n",
        "    for utterance in dialoge[\"utterances\"]:\n",
        "      history = utterance[\"history\"][-(2*params[\"num_history\"]+1):]\n",
        "      for j, candidate in enumerate(utterance[\"candidates\"][-params[\"num_candidates\"]:]):\n",
        "        lm_labels = bool(j==params[\"num_candidates\"]-1)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLE3w35ygkA_",
        "outputId": "99960d49-c2a7-4d29-9ca4-72f6bf370b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "arr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what kind ? holiday parties ? work parties ?',\n",
              " 'i love dogs want a husky but cant have one yet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y73oYAITeM3A"
      },
      "source": [
        "sequence = [['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'],\n",
        " ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
        " ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
        " ['<speaker1>', 'great', 'to', 'hear', '<eos>']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU92LDokmjP5"
      },
      "source": [
        "speaker2 = 'speaker2'\n",
        "speaker1 = 'speaker1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNezuiZolhR1",
        "outputId": "9ee9416e-1dd2-4ecb-b361-f3e80a6b7204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "([-1] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmEDtofsn8DJ"
      },
      "source": [
        "seg = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WetWt820zx"
      },
      "source": [
        "def add_special_token(model,tokenizer):\n",
        "  origin_num_tokens = len(tokenizer.encoder)\n",
        "  num_special_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "  if num_special_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=origin_num_tokens + num_special_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzKolTBxBuE"
      },
      "source": [
        "model = GPT2DoubleHeadsModel.from_pretrained(params[\"model_checkpoint\"])\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(params[\"model_checkpoint\"])\n",
        "model.to(params[\"device\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcS_NATZ11B-"
      },
      "source": [
        "add_special_token(model,tokenizer)\n",
        "optimizer = AdamW(model.parameters(), lr=params[\"lr\"] , correct_bias=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vh7caeyG2oh"
      },
      "source": [
        "if params[\"fp16_training\"]:\n",
        "  from apex import amp\n",
        "  # Allow Amp to perform casts as required by the opt_level \n",
        "  model,optimizer = amp.initialize(model,optimizer,opt_level=params[\"fp16_training\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3sxWcaRjNx"
      },
      "source": [
        "def get_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u08d7LQuJ2r2"
      },
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  start_time = time.time()\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOZjHfN9K07L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}