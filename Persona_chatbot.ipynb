{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persona chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11luyZ0Mcc2p6l2nIFE31uE6nmm0yQXDd",
      "authorship_tag": "ABX9TyPhBrrBTvUAo+nIFpbpK28i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c4373f7e83e4550b2e3ca4d34ba1c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26e97d3791a046f8ae351a78b23b45ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f574b89fb33a4b1188ac259ad08caf29",
              "IPY_MODEL_ddb011daac54497f8767e04cf9711d9b"
            ]
          }
        },
        "26e97d3791a046f8ae351a78b23b45ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f574b89fb33a4b1188ac259ad08caf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd54c91f964243b794647f0e4afec31f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 65719,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 65719,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_338bd440a7644687894f78c60e104b22"
          }
        },
        "ddb011daac54497f8767e04cf9711d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46519cce33874fe181f397dc88e2e230",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 65719/65719 [4:27:55&lt;00:00,  4.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04cbd1b0441a4e448b97d34462ba2e5f"
          }
        },
        "bd54c91f964243b794647f0e4afec31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "338bd440a7644687894f78c60e104b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46519cce33874fe181f397dc88e2e230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04cbd1b0441a4e448b97d34462ba2e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3264432eef1747a6bb915e997ff0a26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_688cb3ec20b64d7fb0b42c0e42767978",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_086105df1c544603884b25db4a1421a4",
              "IPY_MODEL_7aea3036c771455f91a8d67e1c17f349"
            ]
          }
        },
        "688cb3ec20b64d7fb0b42c0e42767978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "086105df1c544603884b25db4a1421a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_621f7c94f6ba4b82aa96542953bae31b",
            "_dom_classes": [],
            "description": "Evaluating:   1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3901,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 55,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1eb3ac6601e84f71aeedc9c928d29313"
          }
        },
        "7aea3036c771455f91a8d67e1c17f349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_471a246858fb4b68bb5efbf6b861ea94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 55/3901 [00:24&lt;28:27,  2.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9e05ce0ba4c4408951c62b2fe8b5b96"
          }
        },
        "621f7c94f6ba4b82aa96542953bae31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1eb3ac6601e84f71aeedc9c928d29313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "471a246858fb4b68bb5efbf6b861ea94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9e05ce0ba4c4408951c62b2fe8b5b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyaoranClone/Persona-Chatbot/blob/master/Persona_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cRfdffuypIT"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4kE9OVyDnyD",
        "outputId": "40d24840-55d4-4e71-a702-d8f04970ba5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#install Apex\n",
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyLh1dN6CIoX"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRc_k_5w4wg"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "from itertools import chain\n",
        "from argparse import ArgumentParser\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm, trange\n",
        "from tqdm import tnrange, notebook\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, DistributedSampler, SequentialSampler\n",
        "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
        "                                 GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLDrxn3u2SAe"
      },
      "source": [
        "!pip install spacy ftfy==4.4.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQZZGVyS3N47"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OIIpnG36uL",
        "outputId": "2ba681a5-03eb-42de-e272-e1f9c8c51ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSBJz8H2IA_"
      },
      "source": [
        "#According to Huggingface Convai tutorial\n",
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
        "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La6zq6rK48c3"
      },
      "source": [
        "#define hyperparameters\n",
        "class args:\n",
        "  model_checkpoint = 'gpt2'\n",
        "  device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  lr  = 6.25e-5\n",
        "  num_candidates = 2 #4\n",
        "  personality_permutations = 1\n",
        "  num_history  = 2 #Number of previous exchanges to keep in history\n",
        "  fp16_training = \"O1\" #Set to O0, O1, O2 or O3 for fp16 training\n",
        "  train_batch_size = 2\n",
        "  valid_batch_size = 2\n",
        "  num_epochs= 2 #3\n",
        "  no_sample=False\n",
        "  lm_coef = 2.0\n",
        "  mc_coef = 1.0\n",
        "  max_norm= 1.0\n",
        "  top_p = 0.9 # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\n",
        "  top_k = 0 # Filter top-k tokens before sampling (<=0: no filtering)\n",
        "  temperature = 0.7 # Sampling softmax temperature\n",
        "  max_len = 20 #Maximum length of the output utterances\n",
        "  min_len = 1\n",
        "  num_gpu = torch.cuda.device_count() #1\n",
        "  gradient_accumulation_steps= 4\n",
        "  local_rank= -1 # for distributed training\n",
        "  url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "  local_path= \"/content/drive/My Drive/Colab Notebooks/Dataset/personachat_self_original.json\"\n",
        "  dataset_cache_path = \"/content/drive/My Drive/Colab Notebooks/Dataset/dataset_cache/persona_cache.bin_GPT2Tokenizer\"\n",
        "  saved_dir = \"/content/drive/My Drive/Colab Notebooks/Trained Models/persona_chatbot/\"\n",
        "\n",
        "args = args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpGazIhUBqU"
      },
      "source": [
        "class FacebookPersonaDataset():\n",
        "  \"\"\"\n",
        "    Concatenate context segments in a single sequence [[bos+persona], [history], [reply+eos]]\n",
        "    Tokenize and convert them to tensor\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self,url,cache_path,tokenizer = ''):\n",
        "    self.file_path = url\n",
        "    self.tokenizer = tokenizer\n",
        "    self.cache_path = cache_path\n",
        "\n",
        "  def load_dataset(self):\n",
        "    #self.cache_path = self.cache_path + '_' + type(self.tokenizer).__name__  # To avoid using GPT cache for GPT-2 and vice-versa\n",
        "    if self.cache_path and os.path.isfile(self.cache_path):\n",
        "      #load tokenized dataset\n",
        "      dataset = torch.load(self.cache_path)\n",
        "      print(\"dataset loaded\")\n",
        "    else:\n",
        "      with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.loads(f.read())\n",
        "      \n",
        "      def tokenize(obj):\n",
        "        if isinstance(obj,str):\n",
        "          return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(obj))\n",
        "        if isinstance(obj,dict):\n",
        "          return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "        return list(tokenize(o) for o in obj)\n",
        "      dataset = tokenize(dataset)\n",
        "      torch.save(dataset,self.cache_path)\n",
        "    return dataset\n",
        "\n",
        "  def _pad_dataset(self,dataset, padding=0):\n",
        "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
        "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "    return dataset\n",
        "\n",
        "  def build_input(self,persona,history,reply,lm_labels = False,with_eos = True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "    #bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
        "    #sequence = [[bos] + list(persona) + [\"his\"] + history + [\"rep\"] + [reply]]\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])] #add bos, eos\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])] #add speaker1, speaker2\n",
        "    #after concat: [[bos+persona], [history], [reply+eos]]\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    #segment\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    #position\n",
        "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "    #language model labels is used to calculate lm_loss\n",
        "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"]) #labels set to -100 are ignored (masked)\n",
        "    if lm_labels:\n",
        "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "    return instance\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "    personachat_dataset = self.load_dataset()\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for name,dataset in personachat_dataset.items():\n",
        "      num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"]) #num_candidates are same for all dialoges\n",
        "      if args.num_candidates > 0 and name == 'train':\n",
        "        num_candidates =  min(num_candidates ,args.num_candidates)\n",
        "      for dialoge in dataset:\n",
        "        persona = dialoge[\"personality\"].copy()\n",
        "        for _ in range(args.personality_permutations):\n",
        "          for utterance in dialoge[\"utterances\"]:\n",
        "            history = utterance[\"history\"][-(2*args.num_history+1):]\n",
        "            for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "              #The last candidate is a ground truth reponse.\n",
        "              lm_labels = bool(j==num_candidates-1)\n",
        "              instance = self.build_input(persona,history,candidate,lm_labels)\n",
        "              for input_name,data in instance.items():\n",
        "                datasets[name][input_name].append(data) #datasets['train']['input_ids'] of [[c1 in u1],[c2 in u1],..,[c1 in u 7][c2 in u7]]\n",
        "            datasets[name][\"mc_labels\"].append(num_candidates - 1) #7\n",
        "            datasets[name][\"n_candidates\"] = num_candidates\n",
        "          persona = [persona[-1]] + persona[:-1]  # permuted personalitie\n",
        "          \n",
        "    #pad input and convert to tensor\n",
        "    print(\"pad input and convert to tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "      dataset =  self._pad_dataset(dataset, padding=self.tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "      for input_name in MODEL_INPUTS:\n",
        "        tensor =  torch.tensor(dataset[input_name])\n",
        "        if input_name != \"mc_labels\":\n",
        "          tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "        tensor_datasets[dataset_name].append(tensor)\n",
        "\n",
        "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    return train_dataset, valid_dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WetWt820zx"
      },
      "source": [
        "def add_special_token(model,tokenizer):\n",
        "  origin_num_tokens = len(tokenizer.encoder)\n",
        "  num_special_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "  if num_special_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=origin_num_tokens + num_special_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzKolTBxBuE"
      },
      "source": [
        "model_class_name = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
        "tokenizer_class_name = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
        "model = model_class_name.from_pretrained(args.model_checkpoint)\n",
        "tokenizer = tokenizer_class_name.from_pretrained(args.model_checkpoint)\n",
        "model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcS_NATZ11B-"
      },
      "source": [
        "add_special_token(model,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u08d7LQuJ2r2"
      },
      "source": [
        "def train(train_dataset,valid_dataset,model,tokenizer):\n",
        "  train_sampler = RandomSampler(train_dataset) if args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  train_loader = DataLoader(train_dataset,batch_size=args.train_batch_size,sampler=train_sampler)\n",
        "  optimizer = AdamW(model.parameters(),lr = args.lr,correct_bias=True)\n",
        "\n",
        "  if args.fp16_training:\n",
        "    from apex import amp\n",
        "    # Allow Amp to perform casts as required by the opt_level \n",
        "    model,optimizer = amp.initialize(model,optimizer,opt_level=args.fp16_training)\n",
        "  #Linearly decrease the learning rate from lr to zero\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = args.num_epochs* len(train_loader))\n",
        "  tr_loss, loss = 0.0, 0.0\n",
        "  global_step = 0\n",
        "  metrics = {\n",
        "      \"nll\" : 1000.0,\n",
        "      \"accuracy\": 0.0\n",
        "  }\n",
        "  model.zero_grad() # Reset gradient tensor\n",
        "  if args.num_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "  if args.local_rank != -1:\n",
        "    model = DistributedDataParallel(model,device_ids = [args.local_rank],output_device=args.local_rank,find_unused_parameters=True)\n",
        "  for _ in range(args.num_epochs):\n",
        "    for step,batch in enumerate(notebook.tqdm(train_loader,disable= args.local_rank not in [-1,0])):\n",
        "      model.train()\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch )\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "\n",
        "      (lm_loss),(mc_loss), *_ = model(input_ids,token_type_ids=token_type_ids,mc_labels=mc_labels,lm_labels=lm_labels)\n",
        "\n",
        "      loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef)/args.gradient_accumulation_steps # Normalize our loss (if averaged)\n",
        "\n",
        "      if args.num_gpu > 1:\n",
        "        # mean() to average on multi-gpu parallel training\n",
        "        loss = loss.mean()\n",
        "\n",
        "      if step % 500 == 0:\n",
        "        print(\"Loss for step {} is {}\".format(step, loss))\n",
        "\n",
        "      if args.fp16_training:\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "          scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n",
        "      else:\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),args.max_norm)\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      global_step += 1\n",
        "      if (step+1) % args.gradient_accumulation_steps == 0: # Wait a several backward step\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "        # if args.local_rank in [-1,0]:\n",
        "        #   metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "      # Update the learning rate.\n",
        "      scheduler.step()\n",
        "    #eval model \n",
        "    if args.local_rank in [-1,0]:\n",
        "      metrics = evaluate(model, valid_dataset, metrics, tokenizer)\n",
        "        \n",
        "  return tr_loss/global_step,metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOZjHfN9K07L"
      },
      "source": [
        "def evaluate(model,valid_dataset,metrics,tokenizer):\n",
        "  valid_sample = SequentialSampler(valid_dataset) if  args.local_rank== -1 else DistributedSampler(train_dataset)\n",
        "  eval_dataloader = DataLoader(valid_dataset,sampler=valid_sample,batch_size=args.valid_batch_size)\n",
        "\n",
        "  print(' * Running Evaluation')\n",
        "  print(' * Num of examples: %d\" ',len(valid_dataset))\n",
        "  print(' * Batch size: %d\" ',args.valid_batch_size)\n",
        "  nlls = None\n",
        "  accs = None\n",
        "  for step,batch in enumerate(notebook.tqdm(eval_dataloader,desc=\"Evaluating\")):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
        "      input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "      lm_logits, mc_logits, *_ = model(\n",
        "          input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids)\n",
        "      \n",
        "      lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))\n",
        "      lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
        "            \n",
        "      x = ((lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels))\n",
        "      #convert pytorch tensor to numpy array to calculate\n",
        "      nll = torch.nn.CrossEntropyLoss(ignore_index=-100)(x[0][0], x[1][0]).detach().cpu().numpy()\n",
        "      acc = torch.sum((torch.max(x[0][1], 1)[1] == x[1][1]).int()).detach().cpu().numpy().mean()\n",
        "\n",
        "    if nlls is None:\n",
        "      nlls = nll\n",
        "      accs = acc\n",
        "    else:\n",
        "      nlls = np.append(nlls, nll)\n",
        "      accs = np.append(accs, acc)\n",
        "  \n",
        "  nlls_mean = np.mean(nlls)\n",
        "  accs_mean = np.mean(accs)\n",
        "    \n",
        "  if accs_mean>metrics['accuracy'] and nlls_mean<metrics['nll']:\n",
        "    print(\" * New high accuracy and nll! {} {} \".format(accs_mean, nlls_mean))\n",
        "    metrics.update({'nll': nlls_mean, 'accuracy': accs_mean})\n",
        "    #Save model if getting high accuracy\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model #Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.saved_dir)\n",
        "    tokenizer.save_pretrained(args.saved_dir)\n",
        "    \n",
        "  return metrics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BhPSl9HHCzB"
      },
      "source": [
        "persona_dataset = FacebookPersonaDataset(args.local_path,args.dataset_cache_path,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tekpUzGE-lEZ",
        "outputId": "0344c195-7170-43c4-c870-c8037935e38c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_dataset, valid_dataset = persona_dataset.get_data_loaders() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded\n",
            "pad input and convert to tensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY8Pw_6oSWsz",
        "outputId": "f2975b22-0fec-4f16-f6f7-2e724a138120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset.tensors[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([131438, 2, 280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHpSqEnMV2mU",
        "outputId": "269d596b-8e12-4365-83cb-2c8c9adc54ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c4373f7e83e4550b2e3ca4d34ba1c04",
            "26e97d3791a046f8ae351a78b23b45ce",
            "f574b89fb33a4b1188ac259ad08caf29",
            "ddb011daac54497f8767e04cf9711d9b",
            "bd54c91f964243b794647f0e4afec31f",
            "338bd440a7644687894f78c60e104b22",
            "46519cce33874fe181f397dc88e2e230",
            "04cbd1b0441a4e448b97d34462ba2e5f",
            "3264432eef1747a6bb915e997ff0a26a",
            "688cb3ec20b64d7fb0b42c0e42767978",
            "086105df1c544603884b25db4a1421a4",
            "7aea3036c771455f91a8d67e1c17f349",
            "621f7c94f6ba4b82aa96542953bae31b",
            "1eb3ac6601e84f71aeedc9c928d29313",
            "471a246858fb4b68bb5efbf6b861ea94",
            "a9e05ce0ba4c4408951c62b2fe8b5b96"
          ]
        }
      },
      "source": [
        "total_loss, metric = train(train_dataset,valid_dataset,model,tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c4373f7e83e4550b2e3ca4d34ba1c04",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=65719.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:905: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss for step 0 is 49.82383728027344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
            "Loss for step 500 is 2.751466751098633\n",
            "Loss for step 1000 is 2.3079042434692383\n",
            "Loss for step 1500 is 2.2074952125549316\n",
            "Loss for step 2000 is 1.8295009136199951\n",
            "Loss for step 2500 is 1.859522819519043\n",
            "Loss for step 3000 is 1.9340152740478516\n",
            "Loss for step 3500 is 1.2224143743515015\n",
            "Loss for step 4000 is 1.6883606910705566\n",
            "Loss for step 4500 is 1.942408800125122\n",
            "Loss for step 5000 is 1.633164882659912\n",
            "Loss for step 5500 is 1.947487473487854\n",
            "Loss for step 6000 is 1.7471046447753906\n",
            "Loss for step 6500 is 1.395079255104065\n",
            "Loss for step 7000 is 2.028625011444092\n",
            "Loss for step 7500 is 1.3378252983093262\n",
            "Loss for step 8000 is 1.3727840185165405\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Loss for step 8500 is 1.8774354457855225\n",
            "Loss for step 9000 is 1.481187105178833\n",
            "Loss for step 9500 is 1.0064831972122192\n",
            "Loss for step 10000 is 1.4014968872070312\n",
            "Loss for step 10500 is 0.9619714021682739\n",
            "Loss for step 11000 is 1.6992323398590088\n",
            "Loss for step 11500 is 1.3452117443084717\n",
            "Loss for step 12000 is 1.5454659461975098\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 12500 is 1.6171900033950806\n",
            "Loss for step 13000 is 1.213475227355957\n",
            "Loss for step 13500 is 1.8046506643295288\n",
            "Loss for step 14000 is 1.3725438117980957\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 14500 is 1.8718265295028687\n",
            "Loss for step 15000 is 2.177483558654785\n",
            "Loss for step 15500 is 2.1624109745025635\n",
            "Loss for step 16000 is 1.605603575706482\n",
            "Loss for step 16500 is 1.5109435319900513\n",
            "Loss for step 17000 is 1.3059258460998535\n",
            "Loss for step 17500 is 1.6286580562591553\n",
            "Loss for step 18000 is 1.362066626548767\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 18500 is 1.9110733270645142\n",
            "Loss for step 19000 is 1.479662537574768\n",
            "Loss for step 19500 is 2.460315465927124\n",
            "Loss for step 20000 is 2.499811887741089\n",
            "Loss for step 20500 is 1.4950923919677734\n",
            "Loss for step 21000 is 0.9068482518196106\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 21500 is 1.3235105276107788\n",
            "Loss for step 22000 is 1.904830813407898\n",
            "Loss for step 22500 is 2.5189208984375\n",
            "Loss for step 23000 is 2.00358247756958\n",
            "Loss for step 23500 is 2.3086962699890137\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 24000 is 1.6055619716644287\n",
            "Loss for step 24500 is 1.6007424592971802\n",
            "Loss for step 25000 is 1.8092771768569946\n",
            "Loss for step 25500 is 2.194911003112793\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 26000 is 1.0840712785720825\n",
            "Loss for step 26500 is 1.1103533506393433\n",
            "Loss for step 27000 is 2.0412421226501465\n",
            "Loss for step 27500 is 0.9809980392456055\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 28000 is 1.8846189975738525\n",
            "Loss for step 28500 is 1.6216803789138794\n",
            "Loss for step 29000 is 1.7261911630630493\n",
            "Loss for step 29500 is 1.585662603378296\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 30000 is 0.7997820973396301\n",
            "Loss for step 30500 is 1.1583704948425293\n",
            "Loss for step 31000 is 1.8385728597640991\n",
            "Loss for step 31500 is 2.4829742908477783\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 32000 is 1.5113332271575928\n",
            "Loss for step 32500 is 1.7924370765686035\n",
            "Loss for step 33000 is 1.3698899745941162\n",
            "Loss for step 33500 is 1.529950499534607\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 34000 is 1.5539023876190186\n",
            "Loss for step 34500 is 0.9583107233047485\n",
            "Loss for step 35000 is 1.4468928575515747\n",
            "Loss for step 35500 is 1.07615327835083\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 36000 is 1.032835841178894\n",
            "Loss for step 36500 is 1.4964030981063843\n",
            "Loss for step 37000 is 2.150545120239258\n",
            "Loss for step 37500 is 1.6096031665802002\n",
            "Loss for step 38000 is 1.692163348197937\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 38500 is 1.4740041494369507\n",
            "Loss for step 39000 is 0.6915820837020874\n",
            "Loss for step 39500 is 1.427908182144165\n",
            "Loss for step 40000 is 1.620088815689087\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 40500 is 2.719142198562622\n",
            "Loss for step 41000 is 0.9374732375144958\n",
            "Loss for step 41500 is 1.658814787864685\n",
            "Loss for step 42000 is 1.0267038345336914\n",
            "Loss for step 42500 is 1.3948538303375244\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 43000 is 1.027824878692627\n",
            "Loss for step 43500 is 1.8977127075195312\n",
            "Loss for step 44000 is 1.5665990114212036\n",
            "Loss for step 44500 is 1.87797212600708\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 45000 is 0.9376209378242493\n",
            "Loss for step 45500 is 1.311885952949524\n",
            "Loss for step 46000 is 2.450087308883667\n",
            "Loss for step 46500 is 1.2008082866668701\n",
            "Loss for step 47000 is 1.5441358089447021\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 47500 is 0.9235641360282898\n",
            "Loss for step 48000 is 1.7728904485702515\n",
            "Loss for step 48500 is 1.786423921585083\n",
            "Loss for step 49000 is 1.8542650938034058\n",
            "Loss for step 49500 is 1.3331010341644287\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 50000 is 1.4525834321975708\n",
            "Loss for step 50500 is 1.3658292293548584\n",
            "Loss for step 51000 is 1.2121422290802002\n",
            "Loss for step 51500 is 1.5637764930725098\n",
            "Loss for step 52000 is 1.302104115486145\n",
            "Loss for step 52500 is 1.400863766670227\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 53000 is 1.4488184452056885\n",
            "Loss for step 53500 is 1.796651840209961\n",
            "Loss for step 54000 is 2.0192861557006836\n",
            "Loss for step 54500 is 1.7717983722686768\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 55000 is 1.3778047561645508\n",
            "Loss for step 55500 is 1.545362114906311\n",
            "Loss for step 56000 is 1.4231007099151611\n",
            "Loss for step 56500 is 1.6269325017929077\n",
            "Loss for step 57000 is 1.7252840995788574\n",
            "Loss for step 57500 is 1.4769235849380493\n",
            "Loss for step 58000 is 1.2257593870162964\n",
            "Loss for step 58500 is 1.7676631212234497\n",
            "Loss for step 59000 is 1.2732598781585693\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Loss for step 59500 is 0.9338159561157227\n",
            "Loss for step 60000 is 1.2977982759475708\n",
            "Loss for step 60500 is 0.896748423576355\n",
            "Loss for step 61000 is 1.1648961305618286\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Loss for step 61500 is 1.8316253423690796\n",
            "Loss for step 62000 is 1.5326558351516724\n",
            "Loss for step 62500 is 1.959670066833496\n",
            "Loss for step 63000 is 0.9591078162193298\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Loss for step 63500 is 0.8451785445213318\n",
            "Loss for step 64000 is 1.2554676532745361\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Loss for step 64500 is 1.9472442865371704\n",
            "Loss for step 65000 is 1.6100099086761475\n",
            "Loss for step 65500 is 1.7463475465774536\n",
            "\n",
            " * Running Evaluation\n",
            " * Num of examples: %d\"  7801\n",
            " * Batch size: %d\"  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3264432eef1747a6bb915e997ff0a26a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=3901.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55VYUukYLQ99"
      },
      "source": [
        "#Interact\n",
        "def top_filtering(logits, top_k = 0, top_p = 0.9,threshold = -float('Inf'),filter_value=-float('Inf')):\n",
        "  \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "            threshold: a minimal threshold to keep logits\n",
        "  \"\"\"\n",
        "  assert logits.dim() == 1 #batch_size = 1\n",
        "  top_k = min(top_k,logits.size(-1))\n",
        "  if top_k > 0:\n",
        "    # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "    indices_to_remove = torch.top_k(logits,top_k)[0][...,-1,None]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  if top_p > 0.0:\n",
        "    # Compute cumulative probabilities of sorted tokens\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    # Remove tokens with cumulative probability above the threshold\n",
        "    sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "    # Shift the indices to the right to keep also the first token above the threshold\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "    # Back to unsorted indices and set them to -infinity\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = filter_value\n",
        "  \n",
        "  indices_to_remove = logits < threshold\n",
        "  logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oA7fgrJ4BJV"
      },
      "source": [
        "def sample_sequence(personality,history,tokenizer,model,persona_dataset,current_output = None):\n",
        "  \"\"\"\n",
        "    Generate reponse from previous reponses\n",
        "  \"\"\"\n",
        "  special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "  if current_output is None:\n",
        "    current_output = []\n",
        "  for i in range (args.max_len):\n",
        "    instance = persona_dataset.build_input(personality,history,current_output,with_eos=False)\n",
        "    input_ids = torch.tensor(instance[\"input_ids\"],device= args.device).unsqueeze(0)\n",
        "    token_type_ids =  torch.tensor(instance[\"token_type_ids\"], device= args.device).unsqueeze(0)\n",
        "    \n",
        "    logits = model(input_ids,token_type_ids = token_type_ids)\n",
        "    if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "      logits = logits[0] \n",
        "    # logits shape (batch_size, num_choices, sequence_length, vocab_size)\n",
        "    logits = logits[0,-1,:]/args.temperature \n",
        "    logits = top_filtering(logits,top_k = args.top_k,top_p = args.top_p)\n",
        "    probs = F.softmax(logits, -1)\n",
        "\n",
        "    prev = torch.topk(probs,1)[1] if args.no_sample else torch.multinomial(probs,1)\n",
        "    \n",
        "    if i < args.min_len and prev.item() in special_tokens_ids:\n",
        "      while prev.item() in special_tokens_ids:\n",
        "        if probs.max().item() == 1:\n",
        "          warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "          break  # avoid infinitely looping over special token\n",
        "      prev = torch.multinomial(probs, num_samples=1)\n",
        "    \n",
        "    if prev.item() in special_tokens_ids:\n",
        "      break\n",
        "    current_output.append(prev.item())\n",
        "  return current_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgpm4NBkD1Nr"
      },
      "source": [
        "dataset = persona_dataset.load_dataset()\n",
        "personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "personality = random.choice(personalities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W41VRCESB5BX"
      },
      "source": [
        "def predict(model,tokenizer,personality,persona_dataset):\n",
        "  print(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\n",
        "  history = []\n",
        "  while True:\n",
        "    raw_text = input(\">>> \")\n",
        "    while not raw_text:\n",
        "      print('Prompt should not be empty!')\n",
        "      raw_text = input(\">>> \")\n",
        "    history.append(tokenizer.encode(raw_text))\n",
        "    with torch.no_grad():\n",
        "      out_ids = sample_sequence(personality, history, tokenizer, model, persona_dataset)\n",
        "    history.append(out_ids)\n",
        "    history = history[-(2*args.num_history+1):]\n",
        "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    print(out_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX7TAP6FqomP"
      },
      "source": [
        "#reload checkpoints and evaluate on test dataset\n",
        "model = OpenAIGPTDoubleHeadsModel.from_pretrained(args.saved_dir)\n",
        "tokenizer =  OpenAIGPTTokenizer.from_pretrained(args.saved_dir, do_lower_case=True)\n",
        "model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9lMJYV8BK6E"
      },
      "source": [
        "personality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-4P59SfRrEc"
      },
      "source": [
        "tokenizer.decode([547, 1362, 544, 2846, 239])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bps13nu3RVpH"
      },
      "source": [
        "personality.append([1128, 17624, 1532, 10591,239])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBMZo6i-GPk-"
      },
      "source": [
        "tokenizer.decode(chain(*personality))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaRlwCHtIlfe"
      },
      "source": [
        "personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "personality = random.choice(personalities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7drz1fXpjlk",
        "outputId": "0bce1b2d-c415-4658-f6cc-fede5dfc21d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predict(model,tokenizer,personality,persona_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected personality: %s i like to place blame on other people even when i know it is my fault. i'm from san fransico. i like to smell my own farts. my beer gut is so huge i'ven t seen my feet in two years. i'm always the one who buys the beers.\n",
            ">>> what is your name ?\n",
            "my name is george, i love to drive my truck.\n",
            ">>> what is your name ?\n",
            "my name is mia, i am a beer truck driver\n",
            ">>> what is your name ?\n",
            "my name is jenny, what is yours?\n",
            ">>> what is your name ?\n",
            "i am named joe, what is your name?\n",
            ">>> what is your name ?\n",
            "i like to drive, you?\n",
            ">>> what is your name ?\n",
            "i like to drink beer, do you?\n",
            ">>> what is your name ?\n",
            "mine is joe.\n",
            ">>> what is your name ?\n",
            "what do you like to do?\n",
            ">>> what is your name ?\n",
            "what do you do for work?\n",
            ">>> what is your name ?\n",
            "i'm a police officer.\n",
            ">>> what is your name ?\n",
            "what do you do?\n",
            ">>> what is your name ?\n",
            "what is your job?\n",
            ">>> what is your name ?\n",
            "i'm a cop.\n",
            ">>> what is your name ?\n",
            "what do you do for a living?\n",
            ">>> what is your name ?\n",
            "i'm in business\n",
            ">>> what is your name ?\n",
            "i am a cop\n",
            ">>> what is your name ?\n",
            "i am a guy who smokes\n",
            ">>> what is your name ?\n",
            "my name is phil.\n",
            ">>> what is your name ?\n",
            "i like to throw things around\n",
            ">>> what is your name ?\n",
            "my name is john.\n",
            ">>> what is your name ?\n",
            "i'm bob.\n",
            ">>> what is your name ?\n",
            "i am sorry, bob.\n",
            ">>> what is your name ?\n",
            "i just got back from the beach.\n",
            ">>> what is your name ?\n",
            "my name is jack.\n",
            ">>> what is your name ?\n",
            "i am jake.\n",
            ">>> what is your name ?\n",
            "i'm gerald.\n",
            ">>> what is your name ?\n",
            "i'm tom. i've a huge beer gut.\n",
            ">>> what is your name ?\n",
            "i'm dave.\n",
            ">>> what is your name ?\n",
            "i'm tim.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-75c43f682c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersonality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersona_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-5de9233daf8c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, tokenizer, personality, persona_dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prompt should not be empty!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykv4heIDMeuP"
      },
      "source": [
        "x = (([1,2,3],[[3,4,3],[5,8,5],[6,6,11]]),([3,4,3],[5,5,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBDJI7pjGKBz"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUMHXeNTMn_p",
        "outputId": "fcb8f9ab-880d-4586-a252-e80f9cf8d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.sum((torch.max(torch.tensor(x[0][1]), 1)[1] == torch.tensor(x[1][1])).int()).detach().cpu().numpy().mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}